{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fafbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "csv_path = \"/content/drive/MyDrive/transactions.csv\"\n",
    "\n",
    "# AE artefact'ları (daha önce kaydetmiştin)\n",
    "ae_model_path = \"/content/drive/MyDrive/improved_autoencoder.h5\"      # adını sen nasıl kaydettiysen ona göre düzelt\n",
    "scaler_path   = \"/content/drive/MyDrive/ae_scaler.pkl\"       # adını sen nasıl kaydettiysen ona göre düzelt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_FEATURES = [\n",
    "    \"amount_ngn\",\n",
    "    \"spending_deviation_score\",\n",
    "    \"velocity_score\",\n",
    "    \"user_avg_txn_amt\",\n",
    "    \"user_std_txn_amt\",\n",
    "    \"txn_hour\",\n",
    "    \"is_night_txn\",\n",
    "    \"user_txn_frequency_24h\",\n",
    "    \"txn_count_last_1h\",\n",
    "    \"avg_gap_between_txns\",\n",
    "    \"device_seen_count\",\n",
    "    \"is_device_shared\",\n",
    "    \"new_device_transaction\",\n",
    "    \"geospatial_velocity_anomaly\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f5d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_BASE_FEATURES = [\n",
    "    \"amount_ngn\",\n",
    "    \"user_avg_txn_amt\",\n",
    "    \"user_std_txn_amt\",\n",
    "    \"user_txn_frequency_24h\",\n",
    "    \"txn_count_last_1h\",\n",
    "    \"txn_count_last_24h\",\n",
    "    \"total_amount_last_1h\",\n",
    "    \"avg_gap_between_txns\",\n",
    "    \"txn_hour\",\n",
    "    \"is_weekend\",\n",
    "    \"is_night_txn\",\n",
    "    \"device_seen_count\",\n",
    "    \"is_device_shared\",\n",
    "    \"ip_seen_count\",\n",
    "    \"is_ip_shared\",\n",
    "    \"new_device_transaction\",\n",
    "    \"geospatial_velocity_anomaly\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380eda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_HYBRID_FEATURES = LGBM_BASE_FEATURES + [\"anomaly_score\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8177059",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "required = set(AE_FEATURES + LGBM_BASE_FEATURES + [\"is_fraud\", \"timestamp\"])\n",
    "missing = sorted([c for c in required if c not in df.columns])\n",
    "print(\"Missing:\", missing[:20], \"...\" if len(missing) > 20 else \"\")\n",
    "assert len(missing) == 0, f\"Missing columns: {missing}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f25576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "split_ratio = 0.8\n",
    "split_idx = int(len(df) * split_ratio)\n",
    "\n",
    "df_train = df.iloc[:split_idx].copy()\n",
    "df_test  = df.iloc[split_idx:].copy()\n",
    "\n",
    "print(\"Train:\", df_train.shape, \"Test:\", df_test.shape)\n",
    "print(\"Train fraud rate:\", df_train[\"is_fraud\"].mean())\n",
    "print(\"Test fraud rate:\", df_test[\"is_fraud\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac467603",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = load_model(ae_model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "print(\"AE loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77926bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_score(df_part: pd.DataFrame, batch_size: int = 4096) -> np.ndarray:\n",
    "    X = df_part[AE_FEATURES].copy()\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    recon = ae_model.predict(X_scaled, batch_size=batch_size, verbose=0)\n",
    "    mse = np.mean(np.square(X_scaled - recon), axis=1)\n",
    "    return mse\n",
    "\n",
    "df_train[\"anomaly_score\"] = compute_anomaly_score(df_train)\n",
    "df_test[\"anomaly_score\"]  = compute_anomaly_score(df_test)\n",
    "\n",
    "print(\"anomaly_score added.\")\n",
    "print(df_train[\"anomaly_score\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc44a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[\"is_fraud\"].astype(int).values\n",
    "y_test  = df_test[\"is_fraud\"].astype(int).values\n",
    "\n",
    "X_train_base = df_train[LGBM_BASE_FEATURES].copy()\n",
    "X_test_base  = df_test[LGBM_BASE_FEATURES].copy()\n",
    "\n",
    "X_train_hyb = df_train[LGBM_HYBRID_FEATURES].copy()\n",
    "X_test_hyb  = df_test[LGBM_HYBRID_FEATURES].copy()\n",
    "\n",
    "print(\"Base:\", X_train_base.shape, X_test_base.shape)\n",
    "print(\"Hybrid:\", X_train_hyb.shape, X_test_hyb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b4fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = (y_train == 0).sum()\n",
    "pos = (y_train == 1).sum()\n",
    "scale_pos_weight = neg / max(pos, 1)\n",
    "print(\"neg:\", neg, \"pos:\", pos, \"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 64,\n",
    "    \"max_depth\": -1,\n",
    "    \"min_data_in_leaf\": 200,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"lambda_l2\": 1.0,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"verbosity\": -1,\n",
    "    \"n_jobs\": -1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f03e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(X_train_base, label=y_train)\n",
    "dvalid = lgb.Dataset(X_test_base, label=y_test, reference=dtrain)\n",
    "\n",
    "baseline_model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[dtrain, dvalid],\n",
    "    valid_names=[\"train\", \"test\"],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(50)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c467bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_h = lgb.Dataset(X_train_hyb, label=y_train)\n",
    "dvalid_h = lgb.Dataset(X_test_hyb, label=y_test, reference=dtrain_h)\n",
    "\n",
    "hybrid_model = lgb.train(\n",
    "    params,\n",
    "    dtrain_h,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[dtrain_h, dvalid_h],\n",
    "    valid_names=[\"train\", \"test\"],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(50)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d4e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test, name=\"model\"):\n",
    "    proba = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "    auc = roc_auc_score(y_test, proba)\n",
    "    ap  = average_precision_score(y_test, proba)\n",
    "\n",
    "    # Threshold seçimi: F1 maksimum\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, proba)\n",
    "    f1s = (2 * precision * recall) / (precision + recall + 1e-12)\n",
    "    best_idx = np.argmax(f1s)\n",
    "    best_thr = thresholds[max(best_idx - 1, 0)] if len(thresholds) else 0.5  # güvenli\n",
    "    y_pred = (proba >= best_thr).astype(int)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    p  = precision_score(y_test, y_pred, zero_division=0)\n",
    "    r  = recall_score(y_test, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"ROC AUC:\", auc)\n",
    "    print(\"PR AUC :\", ap)\n",
    "    print(\"Best thr (F1):\", best_thr)\n",
    "    print(\"F1:\", f1, \"Precision:\", p, \"Recall:\", r)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "    return {\"auc\": auc, \"ap\": ap, \"best_thr\": float(best_thr), \"f1\": f1, \"precision\": p, \"recall\": r}\n",
    "\n",
    "baseline_metrics = evaluate(baseline_model, X_test_base, y_test, \"Baseline (Behavior only)\")\n",
    "hybrid_metrics   = evaluate(hybrid_model, X_test_hyb, y_test, \"Hybrid (Behavior + anomaly_score)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "imp = pd.DataFrame({\n",
    "    \"feature\": X_train_hyb.columns,\n",
    "    \"importance\": hybrid_model.feature_importance(importance_type=\"gain\")\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "imp.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b399590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM model\n",
    "joblib.dump(hybrid_model, \"/content/lgbm_model.pkl\")\n",
    "\n",
    "# training metrics (tez için sakla)\n",
    "report = {\n",
    "    \"baseline\": baseline_metrics,\n",
    "    \"hybrid\": hybrid_metrics,\n",
    "    \"lgbm_base_features\": LGBM_BASE_FEATURES,\n",
    "    \"lgbm_hybrid_features\": LGBM_HYBRID_FEATURES,\n",
    "    \"ae_features\": AE_FEATURES\n",
    "}\n",
    "joblib.dump(report, \"/content/training_report.pkl\")\n",
    "\n",
    "print(\"Saved: lgbm_model.pkl, training_report.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"/content/lgbm_model.pkl\")\n",
    "files.download(\"/content/training_report.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
